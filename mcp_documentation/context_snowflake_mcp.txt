Skip to content
Introducing Snowflake Managed MCP Servers for Secure, Governed Data Agents
mcp
We are excited to announce the availability of Snowflake’s managed Model Context Protocol (MCP) servers in public preview, giving AI agents an open-standards-based interface to connect with AI-ready data in Snowflake. Snowflake’s managed MCP servers eliminate integration complexity and management overhead. Customers can connect their Snowflake data with a variety of agentic applications from providers such as Anthropic, CrewAI and Cursor through MCP connectors to create context-rich AI agents and apps. Customers can also include data from partners such as The Washington Post, MSCI, NASDAQ and The Associated Press in their MCP servers.

Customers can now create a managed MCP server alongside their Snowflake data, allowing them to seamlessly retrieve insights from both structured and unstructured data, all while remaining within Snowflake’s secure governance boundary. This approach simplifies the application architecture because the Snowflake-managed MCP server allows AI agents to securely retrieve data from Snowflake accounts without needing to deploy separate infrastructure or build custom integrations. As a result, enterprises can expedite the delivery of generative AI applications on their Snowflake data with richer insights on a standards-based, secure and robust governance model.

This capability brings several key benefits to customers:

Simplified interoperability with the broader agentic AI ecosystem, including agentic platforms such as Anthropic, CrewAI, Cursor, Salesforce and IDE plugins. 

Standards-based interface for agents to discover and invoke tools and retrieve structured and unstructured data. 

Consistent governance across enterprise data, AI tools and now the MCP server, all within the Snowflake secure perimeter. 

Comprehensive authentication with Snowflake’s built-in OAuth service to enable OAuth-based authentication for MCP integrations.

Trusted data from top content providers with proper attribution via Snowflake Cortex Knowledge Extensions, enabling domain-specific and context-aware insights.

With Snowflake-managed MCP servers, agents can be easily configured to interoperate without custom integrations or disparate protocols. Also, developers can streamline governance and authentication across enterprise data and AI applications.

"Enterprises are moving from AI pilots to production, but until now, safely connecting AI to proprietary data has been a critical barrier," said Jonathan Pelosi, Head of Industry, Financial Services, Anthropic. "Our partnership with Snowflake helps solve this by using MCP to connect each organization's governed data directly to Claude. Customers can now use Claude's advanced reasoning on both structured analytics and unstructured documents via Cortex Analyst and Cortex Search, while maintaining enterprise security standards. With Claude and Snowflake, our joint customers are turning proprietary data into competitive advantage."

The Snowflake-managed MCP server
The Snowflake-managed MCP server allows AI agents to securely retrieve data from Snowflake accounts without needing to deploy separate infrastructure. MCP clients discover and invoke tools and retrieve data required for the application. At launch, the Snowflake MCP server includes Snowflake Cortex Analyst and Snowflake Cortex Search as tools on the standards-based interface. Cortex Analyst translates natural language requests into SQL queries that run against governed data to give insights into structured data. Cortex Search enables semantic search and retrieval from unstructured documents stored in or indexed by Snowflake. Customers can also retrieve data from Cortex Knowledge Extensions in Snowflake Marketplace to get licensed content from top publishers such as The Associated Press or The Washington Post. In the near future, we will support Cortex Agents in the MCP server so that remote applications can invoke agents as tools. Customers can define the tools in different database schemas inline with their current policies and access controls. Further, they can have multiple MCP servers in the account, scoped to a specific use case. Such flexible configuration allows customers to incorporate MCP servers into the Snowflake AI Data Cloud without significant changes to their governance model and deliver a high-accuracy and performant agentic experience.   

By providing managed MCP servers based on open source, community-based standards, Snowflake enables customers to adopt MCP on their own infrastructure, extending choice while preserving security.

The role of MCP in evolving enterprise application architecture
Agents can reason and help solve problems dynamically and are transforming the application architecture. Instead of rigid API contracts and restrictive UIs, agents allow for flexible natural language experience, semantic interfaces with appropriate tool use. This evolution from rigid microservices to an agentic architecture will allow reimagined experiences and new applications. But the success of these applications depends on the quality of the data they can access. Agents need easy access to quality data from external systems for accurate context. MCP enables this access using an open-standard protocol that allows agents and external systems to communicate. For enterprises, this means AI agents can be deployed faster, connected to more systems and governed consistently across the stack.

model
MCP overview 
MCP is built on host-client-server architecture. Hosts are AI applications such as Claude Desktop that provide the environment for running agents. Clients are components inside those hosts that maintain direct connections to servers. Lastly, servers surface tools and resources for the agent to use. Tools are executable functions such as querying a database or performing a task.

model-context
This design creates a predictable, open interface for connecting AI agents to various systems of record. Instead of bestowed connectors, AI agents discover available tools via standard endpoints, invoke them with structured inputs and receive results in a consistent format. MCP makes agents plug-and-play in enterprise environments, simplifying integration and improving reliability.

“The intelligence of any AI coding assistant is fundamentally defined by the context it can access," said Ricky Doar, Head of Field Engineering, Cursor. “A managed MCP server like Snowflake's is a rich live data environment for tools like Cursor to consume essential data context and write faster, more accurate, and more secure production-ready code."

The MCP server on Snowflake: Easier and better governed 
By embedding an MCP server directly in Snowflake, customers can easily connect AI agents to their governed enterprise data. Several benefits stand out:

Governance by design: Enforce the same trusted governance policies, from role-based access to masking, for the MCP server as you do for your data. 

Reduced integration effort: With the MCP server, integration happens once. Any compatible agent can then connect without new development, accelerating adoption and reducing maintenance costs.

Extensible framework: Provide agents with access to structured data and unstructured documents. You can refine the tools to improve how agents interact with your data.

Together, these benefits make Snowflake’s MCP server a powerful enabler for enterprises seeking to deploy AI agents while keeping security, governance and trust front and center.

How the Snowflake MCP server works 
Snowflake’s MCP server implements the open MCP specification as a server that surfaces available tools. Customers create an MCP server object and specify the tools and metadata in the server configuration. The server does not require additional compute or incur separate charges. The server object is managed with the same Snowflake role-based access controls (RBACs), ensuring that the same user and group access controls, masking and policies apply. The Snowflake-managed MCP server supports OAuth 2.0 in compliance with MCP protocol requirements.

create or replace
MCP clients that connect with the server, after requisite authentication, are able to discover and invoke these tools. Tool discovery and invocation follow the standard MCP flow. Agents query the /tools/ lists endpoint to discover the tools, and the /tools/call endpoint to invoke the tools. 

Discover tools with the tools/list message:

post
Invoke tools with the tools/call message:

policy
The Snowflake MCP server executes those requests using Snowflake's API and returns results. By combining standards-based interfaces with Snowflake's governance, the Snowflake MCP server delivers a managed, enterprise-ready solution.

Connect to the Snowflake MCP server from Claude.ai
Claude.ai is a next-generation AI assistant built by Anthropic and trained to be safe, accurate and secure to improve productivity. Claude provides expert-level collaboration on the things you need to get done, including critical data analysis. 

To add the Snowflake MCP server in Claude.ai, click on “add custom connector” and navigate to organization connectors. Next, provide the Snowflake MCP server as a custom connector. 

Once the connection is established, you can interact with your Snowflake data from Claude.ai:

custom
By connecting with Snowflake data, Claude can securely retrieve structured and unstructured data — eliminating the need to manually upload files or repeatedly provide context about your business or product. With the Snowflake MCP server, Claude brings you these insights directly, eliminating hours of manual work and letting teams focus on strategic planning instead of information gathering.

“The next wave of enterprise AI hinges upon orchestrating collaborative crews of specialized agents to automate complex processes. For these agentic workflows to succeed in the enterprise, they must be grounded in secure, high-quality data,” said João Moura, Co-Founder and CEO, CrewAI. “Snowflake's launch of a managed MCP Server provides the essential, secure pipeline for our agent crews to access, analyze, and act upon governed data within the AI Data Cloud. For our joint customers, this partnership moves multi-agent systems from a theoretical concept to a practical, enterprise-ready reality, and we are thrilled to be a launch partner.”

Use Cortex Knowledge Extensions in your MCP server
Snowflake Cortex Knowledge Extensions enable AI applications, with proprietary context and knowledge from third-party providers and publishers, together with intellectual property protection and proper attribution for content owners. Cortex Knowledge Extensions are available on Snowflake Marketplace from top-tier providers such as The Associated Press, The Washington Post, Gannett | USA TODAY Network, Stack Overflow, Packt Publishing and PubMed (published by Snowflake).

Once a Cortex Knowledge Extension has been installed in your Snowflake account, it can be added as a Cortex Search service tool in your MCP server:

mcp
Getting started with a Snowflake MCP server 
The Snowflake MCP server is available in public preview today with resources to help customers get started quickly. Setting up a server involves four main steps:

Create the tools and ensure they have requisite permissions

Create the MCP server object with the tools listed in the specification 

Set up authentication with the security integration and client secrets for the client

Use a client such as Claude.ai to connect with the Snowflake MCP server endpoint 

You can now process natural language queries with Cortex Analyst or document retrieval via Cortex Search. Many customers begin with simple read-only use cases before expanding into workflows that include user-defined functions (UDFs) or stored procedures for approved actions.


On October 2, 2025 Snowflake released a managed MCP Server feature Cortex Analyst and Cortex Search. In this article, we’ll explore using this new feature to serve up Snowflake’s documentation to Cursor a better AI-assisted coding experience.
AI Coding assistants like Cursor are becoming commonplace, and for good reason. They can help people who aren’t developers do things they’ve never dreamed possible and can help experienced developers get more done. It’s an incredible tool that’s changed the way I work. The LLMs that power the coding experience are extremely helpful and capable for helping create SQL statements, functions, and procedures. However, they’re not perfect, and that’s partially because but they don’t know everything about Snowflake. They often uses incorrect syntax and sometimes just make up commands or function arguments.

Like with any other AI system, providing task-specific content increases the accuracy and usefulness of the responses. In this article, we’ll install a Cortex Search Service that contains Snowflake’s Documentation into our Snowflake account, configure an MCP server, and connect Cursor to that MCP server so that it can better assist us with code that we’ll run in Snowflake.

The Problem
To illustrate the problem, let’s try to create a simple Snowflake Scripting stored procedure that creates a date dimension table using Cursor. Without any MCP server enabled, I fed it this prompt.

Help me write a Snowflake scripting stored procedure. 
The procedure needs to do the following. 
* The procedure should take two arguments. The first is a table name, the second is a number of days. 
* The procedure should create a table that has a column for date that begins with today's date and has each date for the number of days sepcified in the second argument to the procedure. 
* It should also calculate various attributes of the date such as day of year, day of month, year, month, day of week, day of week number. Calculate as many attributes as you can think of. 
* If the table exists, it should replace the table. 
* the return value should be a string and include the table name, the min date, and the max date.
I ran this a few times and the results gave me good starting points, but they did contain errors that I needed to correct (or ask Cursor to try to correct with varying success). A couple of issues I noticed were mishandling parentheses around conditional logic and failing using the colon in front of bind variables in SQL statements.


Additional iterations used an invalid date/time part in the EXTRACT function and the wrong number of arguments for the DATE_TRUNC function.



It took me less time to work through these errors and get a working procedure than it would have taken me to start from scratch. But what if we could easily provide them with all of the context that’s contained in the Snowflake documentation? That’s exactly what we’re about to do.

Cortex Knowledge Extensions
Snowflake makes its documentation available as a Cortex Search service via the free Snowflake Documentation Cortex Knowledge Extension. If you’re not familiar, Cortex Search is Snowflake’s hybrid retrieval service for unstructured data, and Cortex Knowledge Extensions are Cortex Search Services that are shared via the Snowflake Marketplace. What’s great about Cortex Knowledge Extension is that the provider has already done the work of parsing and chunking the data so that it’s ready to query.

It takes just a minute to make all of the knowledge available in the Snowflake Documentation easy queryable using natural language.

Search Snowflake Marketplace for “Snowflake Documentation.”
From the page for the Snowflake Documentation Cortex Knowledge Extension, choose “Get”. This will bring up the prompt below to confgure the name of the database to be created and which roles should have access. I’ve granted access to the PUBLIC role in my account so everyone can have access.
Press enter or click to view image in full size

Once you have the Cortex Knowledge Extension you can easily add it to Snowflake Intelligence to get documentation-grounded answers to natural language questions about Snowflake. For our use case, we want to expose the documentation to Cursor so it can help us write code more efficiently.

Create Snowflake-managed MCP Server
We’ll now set up our MCP server in our Snowflake account. To do this, we need to configure the MCP server and configure a programmiatic Access Token PAT for the user that will be connecting.

First, we’ll configure the MCP server. This is a simple example that exposes just one tool. In practice, you can have multiple tools in a single MCP server, or configure multiple MCP servers and use Snowflake-native Role-Based Access Control to grant access to specific MCP servers for specific roles.

-- create a new database for our MCP server
CREATE DATABASE IF NOT EXISTS MCP;
CREATE SCHEMA IF NOT EXISTS MCP.MCP_SERVERS;

-- create mcp server
CREATE OR REPLACE MCP SERVER MCP.MCP_SERVERS.SNOWFLAKE_DOCS FROM SPECIFICATION 
$$
  tools:
    - name: "snowflake-docs"
      type: "CORTEX_SEARCH_SERVICE_QUERY"
      identifier: "snowflake_documentation.shared.cke_snowflake_docs_service"
      description: "cortex search service for snowflake documentation"
      title: "Snowflake Docs"
$$;

-- create restrictive role to just access docs search service
CREATE ROLE IF NOT EXISTS SNOWFLAKE_DOCS_ROLE;

GRANT ROLE SNOWFLAKE_DOCS_ROLE TO USER <USERNAME>;
GRANT USAGE ON DATABASE MCP TO ROLE SNOWFLAKE_DOCS_ROLE;
GRANT USAGE ON SCHEMA MCP_SERVERS TO ROLE SNOWFLAKE_DOCS_ROLE;
GRANT USAGE ON MCP SERVER MCP.MCP_SERVERS.SNOWFLAKE_DOCS TO ROLE SNOWFLAKE_DOCS_ROLE;
Next, we’ll create a Programmatic Access Token (PAT) for the user ID we’ll be using to connect. This will return a PAT we’ll use later, so make sure to save it. It’s worth noting that you need to have a network policy in place to use PATs in Snowflake. See the PAT documentation for more details.

ALTER USER <USERNAME> ADD PROGRAMMATIC ACCESS TOKEN CURSOR_DOCS
ROLE_RESTRICTION = 'SNOWFLAKE_DOCS_ROLE'
DAYS_TO_EXPIRY = 90;
Configure Cursor
Now that we’ve set up our MCP server and OAUTH integration in Snowflake we can configure Cursor to connect to it. From the Cursor settings screen, choose Tools & MCP from the side menu and choose Add Custom MCP.

Press enter or click to view image in full size

This will open a text editor to modify a file called mcp.json. If you already have MCP servers configured in Cursor, you can add another entry with the mcpServers section. If you don’t have any configured, you can just copy and paste this section and replace the ACCOUNT_URL with your own (https://orgname-accountname.snowflakecomputing.com) and update the Authorization section. If you chose a different database, schema, or name for your MCP server, you’ll also need to update it here.

{
  "mcpServers": {
    "snowflake-docs": {
      "url": "<ACCOUNT_URL>/api/v2/databases/MCP/schemas/MCP_SERVERS/mcp-servers/SNOWFLAKE_DOCS",
      "headers": {
        "Authorization": "Bearer <PAT>"
      }
    }
  }
}
Once you’ve added the MCP server, you can hover over the snowflake-docs tool to see the tool details. The tool is disabled by default, so you’ll need to click the name of the tool to enable it.

Press enter or click to view image in full size

Testing it Out
Now that you’ve configured Cursor to connect to our MCP server and enabled the snowflake-docs tool, we can test it out. I fed the same prompt that I used earlier into Cursor’s AI pane. This time I received a prompt to ask permission to run our snowflake-docs tool or add it to the allowlist. Adding it to the allowlist will keep us from having to give it permission to run each time.

You can see below that with access to Snowflake docs it’s validating syntax for stored procedures, date functions, and how to use EXECUTE IMMEDIATE in Snowflake Scripting as it builds.

Press enter or click to view image in full size

After Cursor finished building, I ran the code to create and execute the procedure I received no errors. It worked perfectly on the first try. You can see the procedure code that it created below.

CREATE OR REPLACE PROCEDURE create_date_dimension(
    table_name VARCHAR,
    num_days INTEGER
)
RETURNS VARCHAR
LANGUAGE SQL
COMMENT = 'Creates a date dimension table with comprehensive date attributes starting from today for the specified number of days'
EXECUTE AS CALLER
AS
$$
DECLARE
    sql_statement VARCHAR(4000);
    min_date DATE;
    max_date DATE;
    result_message VARCHAR(500);
BEGIN
    -- Calculate min and max dates
    min_date := CURRENT_DATE();
    max_date := DATEADD(day, num_days - 1, CURRENT_DATE());
    
    -- Build the dynamic SQL statement to create the date dimension table
    sql_statement := 'CREATE OR REPLACE TABLE ' || table_name || ' AS ' ||
        'WITH date_sequence AS ( ' ||
            'SELECT DATEADD(day, ROW_NUMBER() OVER (ORDER BY 1) - 1, CURRENT_DATE()) AS date_value ' ||
            'FROM TABLE(GENERATOR(ROWCOUNT => ' || num_days || ')) ' ||
        ') ' ||
        'SELECT ' ||
            'date_value AS date_column, ' ||
            -- Basic date components
            'YEAR(date_value) AS year, ' ||
            'MONTH(date_value) AS month, ' ||
            'DAY(date_value) AS day, ' ||
            'QUARTER(date_value) AS quarter, ' ||
            
            -- Day-related attributes
            'DAYOFYEAR(date_value) AS day_of_year, ' ||
            'DAYOFWEEK(date_value) AS day_of_week_number, ' ||
            'DAYOFWEEKISO(date_value) AS day_of_week_iso, ' ||
            'DAYOFMONTH(date_value) AS day_of_month, ' ||
            'DAYNAME(date_value) AS day_name_short, ' ||
            
            -- Week-related attributes
            'WEEK(date_value) AS week_of_year, ' ||
            'WEEKISO(date_value) AS week_of_year_iso, ' ||
            'YEAROFWEEK(date_value) AS year_of_week, ' ||
            'YEAROFWEEKISO(date_value) AS year_of_week_iso, ' ||
            
            -- Month-related attributes
            'MONTHNAME(date_value) AS month_name, ' ||
            'LAST_DAY(date_value) AS last_day_of_month, ' ||
            'DATEDIFF(day, DATE_TRUNC(''month'', date_value), date_value) + 1 AS day_of_month_calculated, ' ||
            'DATEDIFF(day, DATE_TRUNC(''month'', date_value), LAST_DAY(date_value)) + 1 AS days_in_month, ' ||
            
            -- Additional useful attributes
            'DATE_TRUNC(''month'', date_value) AS first_day_of_month, ' ||
            'DATE_TRUNC(''quarter'', date_value) AS first_day_of_quarter, ' ||
            'DATE_TRUNC(''year'', date_value) AS first_day_of_year, ' ||
            'LAST_DAY(date_value, ''quarter'') AS last_day_of_quarter, ' ||
            'LAST_DAY(date_value, ''year'') AS last_day_of_year, ' ||
            
            -- Business day indicators
            'CASE WHEN DAYOFWEEK(date_value) IN (1, 7) THEN FALSE ELSE TRUE END AS is_weekday, ' ||
            'CASE WHEN DAYOFWEEK(date_value) IN (1, 7) THEN TRUE ELSE FALSE END AS is_weekend, ' ||
            
            -- Relative date indicators
            'CASE WHEN date_value = CURRENT_DATE() THEN TRUE ELSE FALSE END AS is_today, ' ||
            'CASE WHEN date_value = DATEADD(day, -1, CURRENT_DATE()) THEN TRUE ELSE FALSE END AS is_yesterday, ' ||
            'CASE WHEN date_value = DATEADD(day, 1, CURRENT_DATE()) THEN TRUE ELSE FALSE END AS is_tomorrow, ' ||
            
            -- Formatted date strings
            'TO_VARCHAR(date_value, ''YYYY-MM-DD'') AS date_string_iso, ' ||
            'TO_VARCHAR(date_value, ''MM/DD/YYYY'') AS date_string_us, ' ||
            'TO_VARCHAR(date_value, ''DD/MM/YYYY'') AS date_string_eu, ' ||
            'TO_VARCHAR(date_value, ''Month DD, YYYY'') AS date_string_long, ' ||
            'TO_VARCHAR(date_value, ''DY, MON DD, YYYY'') AS date_string_full, ' ||
            
            -- Epoch and timestamp conversions
            'DATEDIFF(day, ''1970-01-01'', date_value) AS days_since_epoch, ' ||
            'DATE_PART(epoch_second, date_value::TIMESTAMP) AS epoch_seconds, ' ||
            
            -- Calendar attributes
            'CASE WHEN DAY(date_value) = 1 THEN TRUE ELSE FALSE END AS is_first_day_of_month, ' ||
            'CASE WHEN date_value = LAST_DAY(date_value) THEN TRUE ELSE FALSE END AS is_last_day_of_month, ' ||
            'CASE WHEN DATE_TRUNC(''quarter'', date_value) = date_value THEN TRUE ELSE FALSE END AS is_first_day_of_quarter, ' ||
            'CASE WHEN LAST_DAY(date_value, ''quarter'') = date_value THEN TRUE ELSE FALSE END AS is_last_day_of_quarter, ' ||
            'CASE WHEN DATE_TRUNC(''year'', date_value) = date_value THEN TRUE ELSE FALSE END AS is_first_day_of_year, ' ||
            'CASE WHEN LAST_DAY(date_value, ''year'') = date_value THEN TRUE ELSE FALSE END AS is_last_day_of_year ' ||
            
        'FROM date_sequence ' ||
        'ORDER BY date_value';
    
    -- Execute the dynamic SQL to create the table
    EXECUTE IMMEDIATE sql_statement;
    
    -- Build the result message
    result_message := 'Table "' || table_name || '" created successfully. ' ||
                     'Date range: ' || min_date::VARCHAR || ' to ' || max_date::VARCHAR || 
                     ' (' || num_days || ' days total)';
    
    RETURN result_message;
END;
$$;

-- Example usage:
CALL create_date_dimension('my_date_dimension', 365);
-- This will create a table called 'my_date_dimension' with 365 days starting from today

-- To verify the procedure works, you can run:
SELECT * FROM my_date_dimension LIMIT 10;
Conclusion
Just like any other AI system, coding assistants benefit greatly from access to accurate context. The Snowflake Documentation Knowledge Extension served up using Snowflake’s MCP server makes it easy to expose Snowflake’s documentation to Cursor, allowing it to make more accurate outputs without user intervention and and helping developers to get more done in less time.